{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84Q8JfvaeZZ6"
   },
   "source": [
    "## Linear Classifier in TensorFlow \n",
    "Using Low Level API in Eager Execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msrikanta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mjtb-EMcm5K0"
   },
   "outputs": [],
   "source": [
    "#Enable Eager Execution if using tensflow version < 2.0\n",
    "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiObW4V4SIOz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8pWsNQOB6X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851264, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-06 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 00:00:00</td>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date symbol        open       close         low        high  \\\n",
       "0  2016-01-05 00:00:00   WLTW  123.430000  125.839996  122.309998  126.250000   \n",
       "1  2016-01-06 00:00:00   WLTW  125.239998  119.980003  119.940002  125.540001   \n",
       "2  2016-01-07 00:00:00   WLTW  116.379997  114.949997  114.930000  119.739998   \n",
       "3  2016-01-08 00:00:00   WLTW  115.480003  116.620003  113.500000  117.440002   \n",
       "4  2016-01-11 00:00:00   WLTW  117.010002  114.970001  114.089996  117.330002   \n",
       "\n",
       "      volume  \n",
       "0  2163600.0  \n",
       "1  2386400.0  \n",
       "2  2489500.0  \n",
       "3  2006300.0  \n",
       "4  1408600.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 7 columns):\n",
      "date      851264 non-null object\n",
      "symbol    851264 non-null object\n",
      "open      851264 non-null float64\n",
      "close     851264 non-null float64\n",
      "low       851264 non-null float64\n",
      "high      851264 non-null float64\n",
      "volume    851264 non-null float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['date','symbol'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "56bad82a-f271-415a-e0d6-cbe1c4290743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
    "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "data_sample = data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msrikanta\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_sample['volume'] = data_sample['volume'] / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2.1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2.3864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2.4895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1.4086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high  volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2.1636\n",
       "1  125.239998  119.980003  119.940002  125.540001  2.3864\n",
       "2  116.379997  114.949997  114.930000  119.739998  2.4895\n",
       "3  115.480003  116.620003  113.500000  117.440002  2.0063\n",
       "4  117.010002  114.970001  114.089996  117.330002  1.4086"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LE4U8lTdQJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_sample.drop('volume',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_sample['volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYK-aUuLbrz2"
   },
   "source": [
    "#### Convert Training and Test Data to numpy float32 arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-S0tQGcncz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_arr = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_arr = Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_arr, Y_arr, test_size=0.30, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im1ZegbDdKgv"
   },
   "source": [
    "### Normalize the data\n",
    "You can use Normalizer from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm = Normalizer()\n",
    "X_train_n = Norm.fit_transform(X_train)\n",
    "X_test_n = Norm.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the Model in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "#W = tf.Variable(tf.zeros(shape=[4,1]), name=\"Weights\")\n",
    "#b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.random_normal([4,1])\n",
    "b = tf.random_normal([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define a function to calculate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n = np.float32(X_train_n)\n",
    "X_test_n = np.float32(X_test_n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "#y = tf.add(tf.matmul(X_train_n,W),b,name='output')\n",
    "def prediction(x, w, b):\n",
    "    xw_matmul = tf.matmul(x, w)\n",
    "    y = tf.add(xw_matmul, b)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "#loss = tf.reduce_mean(tf.square(y-y_train),name='Loss')\n",
    "def loss(y_actual, y_predicted):\n",
    "    diff = y_actual - y_predicted\n",
    "    sqr = tf.square(diff)\n",
    "    avg = tf.reduce_mean(sqr)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Function to train the Model\n",
    "\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss)\n",
    "def train(x, y_actual, w, b, learning_rate=0.03):    \n",
    "    #Record math ops on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch([w,b])    \n",
    "        current_prediction = prediction(x, w, b)\n",
    "        current_loss = loss(y_actual, current_prediction)\n",
    "    #Calculate Gradients for Loss w.r.t Weights and Bias\n",
    "    dw, db = t.gradient(current_loss,[w, b])\n",
    "    #Update Weights and Bias\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model for 100 epochs \n",
    "1. Observe the training loss at every iteration\n",
    "2. Observe Test loss at every 5th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= Tensor(\"sub_4:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_5:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 0 Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_8:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_9:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 1 Tensor(\"Mean_4:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_12:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_13:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 2 Tensor(\"Mean_6:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_16:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_17:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 3 Tensor(\"Mean_8:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_20:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_21:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 4 Tensor(\"Mean_10:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_24:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_25:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 5 Tensor(\"Mean_12:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_28:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_29:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 6 Tensor(\"Mean_14:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_32:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_33:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 7 Tensor(\"Mean_16:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_36:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_37:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 8 Tensor(\"Mean_18:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_40:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_41:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 9 Tensor(\"Mean_20:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_44:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_45:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 10 Tensor(\"Mean_22:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_48:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_49:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 11 Tensor(\"Mean_24:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_52:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_53:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 12 Tensor(\"Mean_26:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_56:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_57:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 13 Tensor(\"Mean_28:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_60:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_61:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 14 Tensor(\"Mean_30:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_64:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_65:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 15 Tensor(\"Mean_32:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_68:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_69:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 16 Tensor(\"Mean_34:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_72:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_73:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 17 Tensor(\"Mean_36:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_76:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_77:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 18 Tensor(\"Mean_38:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_80:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_81:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 19 Tensor(\"Mean_40:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_84:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_85:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 20 Tensor(\"Mean_42:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_88:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_89:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 21 Tensor(\"Mean_44:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_92:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_93:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 22 Tensor(\"Mean_46:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_96:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_97:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 23 Tensor(\"Mean_48:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_100:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_101:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 24 Tensor(\"Mean_50:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_104:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_105:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 25 Tensor(\"Mean_52:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_108:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_109:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 26 Tensor(\"Mean_54:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_112:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_113:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 27 Tensor(\"Mean_56:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_116:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_117:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 28 Tensor(\"Mean_58:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_120:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_121:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 29 Tensor(\"Mean_60:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_124:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_125:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 30 Tensor(\"Mean_62:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_128:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_129:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 31 Tensor(\"Mean_64:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_132:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_133:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 32 Tensor(\"Mean_66:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_136:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_137:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 33 Tensor(\"Mean_68:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_140:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_141:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 34 Tensor(\"Mean_70:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_144:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_145:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 35 Tensor(\"Mean_72:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_148:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_149:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 36 Tensor(\"Mean_74:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_152:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_153:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 37 Tensor(\"Mean_76:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_156:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_157:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 38 Tensor(\"Mean_78:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_160:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_161:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 39 Tensor(\"Mean_80:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_164:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_165:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 40 Tensor(\"Mean_82:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_168:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_169:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 41 Tensor(\"Mean_84:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_172:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_173:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 42 Tensor(\"Mean_86:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_176:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_177:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 43 Tensor(\"Mean_88:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_180:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_181:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 44 Tensor(\"Mean_90:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_184:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_185:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 45 Tensor(\"Mean_92:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_188:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_189:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 46 Tensor(\"Mean_94:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_192:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_193:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 47 Tensor(\"Mean_96:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_196:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_197:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 48 Tensor(\"Mean_98:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= Tensor(\"sub_200:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_201:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 49 Tensor(\"Mean_100:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_204:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_205:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 50 Tensor(\"Mean_102:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_208:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_209:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 51 Tensor(\"Mean_104:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_212:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_213:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 52 Tensor(\"Mean_106:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_216:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_217:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 53 Tensor(\"Mean_108:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_220:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_221:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 54 Tensor(\"Mean_110:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_224:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_225:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 55 Tensor(\"Mean_112:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_228:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_229:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 56 Tensor(\"Mean_114:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_232:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_233:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 57 Tensor(\"Mean_116:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_236:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_237:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 58 Tensor(\"Mean_118:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_240:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_241:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 59 Tensor(\"Mean_120:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_244:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_245:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 60 Tensor(\"Mean_122:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_248:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_249:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 61 Tensor(\"Mean_124:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_252:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_253:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 62 Tensor(\"Mean_126:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_256:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_257:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 63 Tensor(\"Mean_128:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_260:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_261:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 64 Tensor(\"Mean_130:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_264:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_265:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 65 Tensor(\"Mean_132:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_268:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_269:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 66 Tensor(\"Mean_134:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_272:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_273:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 67 Tensor(\"Mean_136:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_276:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_277:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 68 Tensor(\"Mean_138:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_280:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_281:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 69 Tensor(\"Mean_140:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_284:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_285:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 70 Tensor(\"Mean_142:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_288:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_289:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 71 Tensor(\"Mean_144:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_292:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_293:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 72 Tensor(\"Mean_146:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_296:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_297:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 73 Tensor(\"Mean_148:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_300:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_301:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 74 Tensor(\"Mean_150:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_304:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_305:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 75 Tensor(\"Mean_152:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_308:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_309:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 76 Tensor(\"Mean_154:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_312:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_313:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 77 Tensor(\"Mean_156:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_316:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_317:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 78 Tensor(\"Mean_158:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_320:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_321:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 79 Tensor(\"Mean_160:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_324:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_325:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 80 Tensor(\"Mean_162:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_328:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_329:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 81 Tensor(\"Mean_164:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_332:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_333:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 82 Tensor(\"Mean_166:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_336:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_337:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 83 Tensor(\"Mean_168:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_340:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_341:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 84 Tensor(\"Mean_170:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_344:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_345:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 85 Tensor(\"Mean_172:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_348:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_349:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 86 Tensor(\"Mean_174:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_352:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_353:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 87 Tensor(\"Mean_176:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_356:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_357:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 88 Tensor(\"Mean_178:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_360:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_361:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 89 Tensor(\"Mean_180:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_364:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_365:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 90 Tensor(\"Mean_182:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_368:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_369:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 91 Tensor(\"Mean_184:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_372:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_373:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 92 Tensor(\"Mean_186:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_376:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_377:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 93 Tensor(\"Mean_188:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_380:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_381:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 94 Tensor(\"Mean_190:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_384:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_385:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 95 Tensor(\"Mean_192:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_388:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_389:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 96 Tensor(\"Mean_194:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_392:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_393:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 97 Tensor(\"Mean_196:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_396:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_397:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 98 Tensor(\"Mean_198:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= Tensor(\"sub_400:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_401:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 99 Tensor(\"Mean_200:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Training the model for 100 iterations\n",
    "for i in range(100):\n",
    "    W, b = train(X_train_n, y_train, W, b)\n",
    "    print('W=',W,'b=',b)\n",
    "    #print('Current Loss on iteration', i, loss(y_train, prediction(X_train_n, W, b)).numpy()) -- use this with #tf.enable_eager_execution()\n",
    "    print('Current Loss on iteration', i, loss(y_train, prediction(X_train_n, W, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= Tensor(\"sub_400:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_401:0\", shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('W=',W,'b=',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Model Prediction on 1st Examples in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKGvUWahcihp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= Tensor(\"sub_411:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_412:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 0 Tensor(\"Mean_205:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_415:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_416:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 1 Tensor(\"Mean_207:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_419:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_420:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 2 Tensor(\"Mean_209:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_423:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_424:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 3 Tensor(\"Mean_211:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_427:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_428:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 4 Tensor(\"Mean_213:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_431:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_432:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 5 Tensor(\"Mean_215:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_435:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_436:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 6 Tensor(\"Mean_217:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_439:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_440:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 7 Tensor(\"Mean_219:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_443:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_444:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 8 Tensor(\"Mean_221:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_447:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_448:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 9 Tensor(\"Mean_223:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_451:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_452:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 10 Tensor(\"Mean_225:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_455:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_456:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 11 Tensor(\"Mean_227:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_459:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_460:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 12 Tensor(\"Mean_229:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_463:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_464:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 13 Tensor(\"Mean_231:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_467:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_468:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 14 Tensor(\"Mean_233:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_471:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_472:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 15 Tensor(\"Mean_235:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_475:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_476:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 16 Tensor(\"Mean_237:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_479:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_480:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 17 Tensor(\"Mean_239:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_483:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_484:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 18 Tensor(\"Mean_241:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_487:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_488:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 19 Tensor(\"Mean_243:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_491:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_492:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 20 Tensor(\"Mean_245:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_495:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_496:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 21 Tensor(\"Mean_247:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_499:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_500:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 22 Tensor(\"Mean_249:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_503:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_504:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 23 Tensor(\"Mean_251:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_507:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_508:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 24 Tensor(\"Mean_253:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_511:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_512:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 25 Tensor(\"Mean_255:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_515:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_516:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 26 Tensor(\"Mean_257:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_519:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_520:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 27 Tensor(\"Mean_259:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_523:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_524:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 28 Tensor(\"Mean_261:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_527:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_528:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 29 Tensor(\"Mean_263:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_531:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_532:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 30 Tensor(\"Mean_265:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_535:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_536:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 31 Tensor(\"Mean_267:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_539:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_540:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 32 Tensor(\"Mean_269:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_543:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_544:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 33 Tensor(\"Mean_271:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_547:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_548:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 34 Tensor(\"Mean_273:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_551:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_552:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 35 Tensor(\"Mean_275:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_555:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_556:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 36 Tensor(\"Mean_277:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_559:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_560:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 37 Tensor(\"Mean_279:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_563:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_564:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 38 Tensor(\"Mean_281:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_567:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_568:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 39 Tensor(\"Mean_283:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_571:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_572:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 40 Tensor(\"Mean_285:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_575:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_576:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 41 Tensor(\"Mean_287:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_579:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_580:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 42 Tensor(\"Mean_289:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_583:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_584:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 43 Tensor(\"Mean_291:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_587:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_588:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 44 Tensor(\"Mean_293:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_591:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_592:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 45 Tensor(\"Mean_295:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_595:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_596:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 46 Tensor(\"Mean_297:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_599:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_600:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 47 Tensor(\"Mean_299:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_603:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_604:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 48 Tensor(\"Mean_301:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_607:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_608:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 49 Tensor(\"Mean_303:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_611:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_612:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 50 Tensor(\"Mean_305:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= Tensor(\"sub_615:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_616:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 51 Tensor(\"Mean_307:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_619:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_620:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 52 Tensor(\"Mean_309:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_623:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_624:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 53 Tensor(\"Mean_311:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_627:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_628:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 54 Tensor(\"Mean_313:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_631:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_632:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 55 Tensor(\"Mean_315:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_635:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_636:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 56 Tensor(\"Mean_317:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_639:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_640:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 57 Tensor(\"Mean_319:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_643:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_644:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 58 Tensor(\"Mean_321:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_647:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_648:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 59 Tensor(\"Mean_323:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_651:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_652:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 60 Tensor(\"Mean_325:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_655:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_656:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 61 Tensor(\"Mean_327:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_659:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_660:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 62 Tensor(\"Mean_329:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_663:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_664:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 63 Tensor(\"Mean_331:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_667:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_668:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 64 Tensor(\"Mean_333:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_671:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_672:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 65 Tensor(\"Mean_335:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_675:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_676:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 66 Tensor(\"Mean_337:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_679:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_680:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 67 Tensor(\"Mean_339:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_683:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_684:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 68 Tensor(\"Mean_341:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_687:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_688:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 69 Tensor(\"Mean_343:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_691:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_692:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 70 Tensor(\"Mean_345:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_695:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_696:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 71 Tensor(\"Mean_347:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_699:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_700:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 72 Tensor(\"Mean_349:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_703:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_704:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 73 Tensor(\"Mean_351:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_707:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_708:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 74 Tensor(\"Mean_353:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_711:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_712:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 75 Tensor(\"Mean_355:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_715:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_716:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 76 Tensor(\"Mean_357:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_719:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_720:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 77 Tensor(\"Mean_359:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_723:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_724:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 78 Tensor(\"Mean_361:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_727:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_728:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 79 Tensor(\"Mean_363:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_731:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_732:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 80 Tensor(\"Mean_365:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_735:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_736:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 81 Tensor(\"Mean_367:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_739:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_740:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 82 Tensor(\"Mean_369:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_743:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_744:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 83 Tensor(\"Mean_371:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_747:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_748:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 84 Tensor(\"Mean_373:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_751:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_752:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 85 Tensor(\"Mean_375:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_755:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_756:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 86 Tensor(\"Mean_377:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_759:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_760:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 87 Tensor(\"Mean_379:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_763:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_764:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 88 Tensor(\"Mean_381:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_767:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_768:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 89 Tensor(\"Mean_383:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_771:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_772:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 90 Tensor(\"Mean_385:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_775:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_776:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 91 Tensor(\"Mean_387:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_779:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_780:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 92 Tensor(\"Mean_389:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_783:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_784:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 93 Tensor(\"Mean_391:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_787:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_788:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 94 Tensor(\"Mean_393:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_791:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_792:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 95 Tensor(\"Mean_395:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_795:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_796:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 96 Tensor(\"Mean_397:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_799:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_800:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 97 Tensor(\"Mean_399:0\", shape=(), dtype=float32)\n",
      "W= Tensor(\"sub_803:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_804:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 98 Tensor(\"Mean_401:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= Tensor(\"sub_807:0\", shape=(4, 1), dtype=float32) b= Tensor(\"sub_808:0\", shape=(1,), dtype=float32)\n",
      "Current Loss on iteration 99 Tensor(\"Mean_403:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    W, b = train(X_test_n, y_test, W, b)\n",
    "    print('W=',W,'b=',b)\n",
    "    print('Current Loss on iteration', i, loss(y_test, prediction(X_test_n, W, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "## Classification using tf.Keras\n",
    "\n",
    "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow if not done already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0g6lorycihf"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xFvb5sRcihg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      "Id               150 non-null int64\n",
      "SepalLengthCm    150 non-null float64\n",
      "SepalWidthCm     150 non-null float64\n",
      "PetalLengthCm    150 non-null float64\n",
      "PetalWidthCm     150 non-null float64\n",
      "Species          150 non-null object\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.1+ KB\n"
     ]
    }
   ],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAB--Qdwcihm"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJr5dYnocihm"
   },
   "outputs": [],
   "source": [
    "iris_oh = pd.get_dummies(iris,columns = [\"Species\"],drop_first = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species_Iris-setosa</th>\n",
       "      <th>Species_Iris-versicolor</th>\n",
       "      <th>Species_Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>121</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>122</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>123</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>125</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>126</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>127</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>129</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>130</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>131</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>132</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>133</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>134</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>135</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>136</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>137</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>139</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>140</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>141</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>142</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>143</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0      1            5.1           3.5            1.4           0.2   \n",
       "1      2            4.9           3.0            1.4           0.2   \n",
       "2      3            4.7           3.2            1.3           0.2   \n",
       "3      4            4.6           3.1            1.5           0.2   \n",
       "4      5            5.0           3.6            1.4           0.2   \n",
       "5      6            5.4           3.9            1.7           0.4   \n",
       "6      7            4.6           3.4            1.4           0.3   \n",
       "7      8            5.0           3.4            1.5           0.2   \n",
       "8      9            4.4           2.9            1.4           0.2   \n",
       "9     10            4.9           3.1            1.5           0.1   \n",
       "10    11            5.4           3.7            1.5           0.2   \n",
       "11    12            4.8           3.4            1.6           0.2   \n",
       "12    13            4.8           3.0            1.4           0.1   \n",
       "13    14            4.3           3.0            1.1           0.1   \n",
       "14    15            5.8           4.0            1.2           0.2   \n",
       "15    16            5.7           4.4            1.5           0.4   \n",
       "16    17            5.4           3.9            1.3           0.4   \n",
       "17    18            5.1           3.5            1.4           0.3   \n",
       "18    19            5.7           3.8            1.7           0.3   \n",
       "19    20            5.1           3.8            1.5           0.3   \n",
       "20    21            5.4           3.4            1.7           0.2   \n",
       "21    22            5.1           3.7            1.5           0.4   \n",
       "22    23            4.6           3.6            1.0           0.2   \n",
       "23    24            5.1           3.3            1.7           0.5   \n",
       "24    25            4.8           3.4            1.9           0.2   \n",
       "25    26            5.0           3.0            1.6           0.2   \n",
       "26    27            5.0           3.4            1.6           0.4   \n",
       "27    28            5.2           3.5            1.5           0.2   \n",
       "28    29            5.2           3.4            1.4           0.2   \n",
       "29    30            4.7           3.2            1.6           0.2   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "120  121            6.9           3.2            5.7           2.3   \n",
       "121  122            5.6           2.8            4.9           2.0   \n",
       "122  123            7.7           2.8            6.7           2.0   \n",
       "123  124            6.3           2.7            4.9           1.8   \n",
       "124  125            6.7           3.3            5.7           2.1   \n",
       "125  126            7.2           3.2            6.0           1.8   \n",
       "126  127            6.2           2.8            4.8           1.8   \n",
       "127  128            6.1           3.0            4.9           1.8   \n",
       "128  129            6.4           2.8            5.6           2.1   \n",
       "129  130            7.2           3.0            5.8           1.6   \n",
       "130  131            7.4           2.8            6.1           1.9   \n",
       "131  132            7.9           3.8            6.4           2.0   \n",
       "132  133            6.4           2.8            5.6           2.2   \n",
       "133  134            6.3           2.8            5.1           1.5   \n",
       "134  135            6.1           2.6            5.6           1.4   \n",
       "135  136            7.7           3.0            6.1           2.3   \n",
       "136  137            6.3           3.4            5.6           2.4   \n",
       "137  138            6.4           3.1            5.5           1.8   \n",
       "138  139            6.0           3.0            4.8           1.8   \n",
       "139  140            6.9           3.1            5.4           2.1   \n",
       "140  141            6.7           3.1            5.6           2.4   \n",
       "141  142            6.9           3.1            5.1           2.3   \n",
       "142  143            5.8           2.7            5.1           1.9   \n",
       "143  144            6.8           3.2            5.9           2.3   \n",
       "144  145            6.7           3.3            5.7           2.5   \n",
       "145  146            6.7           3.0            5.2           2.3   \n",
       "146  147            6.3           2.5            5.0           1.9   \n",
       "147  148            6.5           3.0            5.2           2.0   \n",
       "148  149            6.2           3.4            5.4           2.3   \n",
       "149  150            5.9           3.0            5.1           1.8   \n",
       "\n",
       "     Species_Iris-setosa  Species_Iris-versicolor  Species_Iris-virginica  \n",
       "0                      1                        0                       0  \n",
       "1                      1                        0                       0  \n",
       "2                      1                        0                       0  \n",
       "3                      1                        0                       0  \n",
       "4                      1                        0                       0  \n",
       "5                      1                        0                       0  \n",
       "6                      1                        0                       0  \n",
       "7                      1                        0                       0  \n",
       "8                      1                        0                       0  \n",
       "9                      1                        0                       0  \n",
       "10                     1                        0                       0  \n",
       "11                     1                        0                       0  \n",
       "12                     1                        0                       0  \n",
       "13                     1                        0                       0  \n",
       "14                     1                        0                       0  \n",
       "15                     1                        0                       0  \n",
       "16                     1                        0                       0  \n",
       "17                     1                        0                       0  \n",
       "18                     1                        0                       0  \n",
       "19                     1                        0                       0  \n",
       "20                     1                        0                       0  \n",
       "21                     1                        0                       0  \n",
       "22                     1                        0                       0  \n",
       "23                     1                        0                       0  \n",
       "24                     1                        0                       0  \n",
       "25                     1                        0                       0  \n",
       "26                     1                        0                       0  \n",
       "27                     1                        0                       0  \n",
       "28                     1                        0                       0  \n",
       "29                     1                        0                       0  \n",
       "..                   ...                      ...                     ...  \n",
       "120                    0                        0                       1  \n",
       "121                    0                        0                       1  \n",
       "122                    0                        0                       1  \n",
       "123                    0                        0                       1  \n",
       "124                    0                        0                       1  \n",
       "125                    0                        0                       1  \n",
       "126                    0                        0                       1  \n",
       "127                    0                        0                       1  \n",
       "128                    0                        0                       1  \n",
       "129                    0                        0                       1  \n",
       "130                    0                        0                       1  \n",
       "131                    0                        0                       1  \n",
       "132                    0                        0                       1  \n",
       "133                    0                        0                       1  \n",
       "134                    0                        0                       1  \n",
       "135                    0                        0                       1  \n",
       "136                    0                        0                       1  \n",
       "137                    0                        0                       1  \n",
       "138                    0                        0                       1  \n",
       "139                    0                        0                       1  \n",
       "140                    0                        0                       1  \n",
       "141                    0                        0                       1  \n",
       "142                    0                        0                       1  \n",
       "143                    0                        0                       1  \n",
       "144                    0                        0                       1  \n",
       "145                    0                        0                       1  \n",
       "146                    0                        0                       1  \n",
       "147                    0                        0                       1  \n",
       "148                    0                        0                       1  \n",
       "149                    0                        0                       1  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_oh.to_csv(\"iris_oh.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D95nY5ILcihj"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyMQoLMucihj"
   },
   "outputs": [],
   "source": [
    "X = iris_oh.drop(['Id','Species_Iris-setosa','Species_Iris-versicolor','Species_Iris-virginica'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = iris_oh.filter([\"Species_Iris-setosa\",\"Species_Iris-versicolor\",\"Species_Iris-virginica\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "0            5.1           3.5            1.4           0.2\n",
       "1            4.9           3.0            1.4           0.2\n",
       "2            4.7           3.2            1.3           0.2\n",
       "3            4.6           3.1            1.5           0.2\n",
       "4            5.0           3.6            1.4           0.2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species_Iris-setosa</th>\n",
       "      <th>Species_Iris-versicolor</th>\n",
       "      <th>Species_Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Species_Iris-setosa  Species_Iris-versicolor  Species_Iris-virginica\n",
       "0                    1                        0                       0\n",
       "1                    1                        0                       0\n",
       "2                    1                        0                       0\n",
       "3                    1                        0                       0\n",
       "4                    1                        0                       0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Divide the dataset into Training and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKGvUWahcihp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b22qpC5xcihr"
   },
   "source": [
    "###  Building Model in tf.keras\n",
    "\n",
    "Build a Linear Classifier model  <br>\n",
    "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
    "2. Apply Softmax on Dense Layer outputs <br>\n",
    "3. Use SGD as Optimizer\n",
    "4. Use categorical_crossentropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hov_UFnUciht"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5FdzqIKcihw"
   },
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qLEdHPscihx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 1.2757 - acc: 0.3429\n",
      "Epoch 2/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.8316 - acc: 0.6571\n",
      "Epoch 3/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.6970 - acc: 0.6952\n",
      "Epoch 4/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.6285 - acc: 0.7238\n",
      "Epoch 5/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.5764 - acc: 0.7429\n",
      "Epoch 6/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.5577 - acc: 0.6667\n",
      "Epoch 7/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.5276 - acc: 0.7333\n",
      "Epoch 8/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.4983 - acc: 0.7905\n",
      "Epoch 9/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.4950 - acc: 0.7524\n",
      "Epoch 10/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.4827 - acc: 0.7333\n",
      "Epoch 11/150\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.4688 - acc: 0.8190\n",
      "Epoch 12/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.4510 - acc: 0.7810\n",
      "Epoch 13/150\n",
      "105/105 [==============================] - 0s 276us/step - loss: 0.4380 - acc: 0.7905\n",
      "Epoch 14/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.4367 - acc: 0.8190\n",
      "Epoch 15/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.4284 - acc: 0.8095\n",
      "Epoch 16/150\n",
      "105/105 [==============================] - 0s 305us/step - loss: 0.4238 - acc: 0.8000\n",
      "Epoch 17/150\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.4110 - acc: 0.8476\n",
      "Epoch 18/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.4000 - acc: 0.8857\n",
      "Epoch 19/150\n",
      "105/105 [==============================] - 0s 286us/step - loss: 0.3862 - acc: 0.8476\n",
      "Epoch 20/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.3933 - acc: 0.8381\n",
      "Epoch 21/150\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.3798 - acc: 0.8476\n",
      "Epoch 22/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.3673 - acc: 0.8571\n",
      "Epoch 23/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.3629 - acc: 0.8857\n",
      "Epoch 24/150\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.3595 - acc: 0.8571\n",
      "Epoch 25/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.3571 - acc: 0.8667\n",
      "Epoch 26/150\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.3485 - acc: 0.8952\n",
      "Epoch 27/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.3536 - acc: 0.8857\n",
      "Epoch 28/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.3401 - acc: 0.8952\n",
      "Epoch 29/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.3222 - acc: 0.8667\n",
      "Epoch 30/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.3290 - acc: 0.9048\n",
      "Epoch 31/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.3225 - acc: 0.9048\n",
      "Epoch 32/150\n",
      "105/105 [==============================] - 0s 428us/step - loss: 0.3030 - acc: 0.9238\n",
      "Epoch 33/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.3025 - acc: 0.9048\n",
      "Epoch 34/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.3034 - acc: 0.9333\n",
      "Epoch 35/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.2958 - acc: 0.9048\n",
      "Epoch 36/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.2939 - acc: 0.9238\n",
      "Epoch 37/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.2888 - acc: 0.9143\n",
      "Epoch 38/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.2861 - acc: 0.9143\n",
      "Epoch 39/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.2844 - acc: 0.9333\n",
      "Epoch 40/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.2704 - acc: 0.9238\n",
      "Epoch 41/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.2630 - acc: 0.9333\n",
      "Epoch 42/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.2625 - acc: 0.9524\n",
      "Epoch 43/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.2650 - acc: 0.9143\n",
      "Epoch 44/150\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.2544 - acc: 0.9429\n",
      "Epoch 45/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.2559 - acc: 0.9048\n",
      "Epoch 46/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.2453 - acc: 0.9429\n",
      "Epoch 47/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.2404 - acc: 0.9333\n",
      "Epoch 48/150\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.2353 - acc: 0.9524\n",
      "Epoch 49/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.2302 - acc: 0.9333\n",
      "Epoch 50/150\n",
      "105/105 [==============================] - 0s 438us/step - loss: 0.2352 - acc: 0.9429\n",
      "Epoch 51/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.2138 - acc: 0.9238\n",
      "Epoch 52/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.2206 - acc: 0.9333\n",
      "Epoch 53/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.2136 - acc: 0.9524\n",
      "Epoch 54/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.2182 - acc: 0.9143\n",
      "Epoch 55/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.2089 - acc: 0.9333\n",
      "Epoch 56/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.2237 - acc: 0.9238\n",
      "Epoch 57/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.2192 - acc: 0.9333\n",
      "Epoch 58/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1983 - acc: 0.9333\n",
      "Epoch 59/150\n",
      "105/105 [==============================] - 0s 476us/step - loss: 0.1984 - acc: 0.9429\n",
      "Epoch 60/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1912 - acc: 0.9524\n",
      "Epoch 61/150\n",
      "105/105 [==============================] - 0s 400us/step - loss: 0.1982 - acc: 0.9524\n",
      "Epoch 62/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1822 - acc: 0.9619\n",
      "Epoch 63/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.1968 - acc: 0.9524\n",
      "Epoch 64/150\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.1939 - acc: 0.9524\n",
      "Epoch 65/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1765 - acc: 0.9524\n",
      "Epoch 66/150\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.1810 - acc: 0.9429\n",
      "Epoch 67/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1849 - acc: 0.9429\n",
      "Epoch 68/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1672 - acc: 0.9619\n",
      "Epoch 69/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1730 - acc: 0.9429\n",
      "Epoch 70/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.1670 - acc: 0.9429\n",
      "Epoch 71/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.1712 - acc: 0.9333\n",
      "Epoch 72/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1791 - acc: 0.9524\n",
      "Epoch 73/150\n",
      "105/105 [==============================] - 0s 305us/step - loss: 0.1731 - acc: 0.9524\n",
      "Epoch 74/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1622 - acc: 0.9524\n",
      "Epoch 75/150\n",
      "105/105 [==============================] - 0s 305us/step - loss: 0.1690 - acc: 0.9524\n",
      "Epoch 76/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.1650 - acc: 0.9429\n",
      "Epoch 77/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1438 - acc: 0.9524\n",
      "Epoch 78/150\n",
      "105/105 [==============================] - 0s 286us/step - loss: 0.1610 - acc: 0.9714\n",
      "Epoch 79/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.1708 - acc: 0.9333\n",
      "Epoch 80/150\n",
      "105/105 [==============================] - 0s 305us/step - loss: 0.1565 - acc: 0.9524\n",
      "Epoch 81/150\n",
      "105/105 [==============================] - 0s 305us/step - loss: 0.1618 - acc: 0.9429\n",
      "Epoch 82/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1431 - acc: 0.9524\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 276us/step - loss: 0.1566 - acc: 0.9429\n",
      "Epoch 84/150\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.1510 - acc: 0.9524\n",
      "Epoch 85/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1523 - acc: 0.9524\n",
      "Epoch 86/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1476 - acc: 0.9619\n",
      "Epoch 87/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1568 - acc: 0.9524\n",
      "Epoch 88/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.1385 - acc: 0.9619\n",
      "Epoch 89/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1399 - acc: 0.9619\n",
      "Epoch 90/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1470 - acc: 0.9429\n",
      "Epoch 91/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1368 - acc: 0.9810\n",
      "Epoch 92/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1341 - acc: 0.9714\n",
      "Epoch 93/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1347 - acc: 0.9524\n",
      "Epoch 94/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.1444 - acc: 0.9429\n",
      "Epoch 95/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1482 - acc: 0.9429\n",
      "Epoch 96/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1377 - acc: 0.9810\n",
      "Epoch 97/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.1365 - acc: 0.9524\n",
      "Epoch 98/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1420 - acc: 0.9524\n",
      "Epoch 99/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1352 - acc: 0.9429\n",
      "Epoch 100/150\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.1278 - acc: 0.9619\n",
      "Epoch 101/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.1315 - acc: 0.9619\n",
      "Epoch 102/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1288 - acc: 0.9429\n",
      "Epoch 103/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.1267 - acc: 0.9619\n",
      "Epoch 104/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.1305 - acc: 0.9524\n",
      "Epoch 105/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.1318 - acc: 0.9524\n",
      "Epoch 106/150\n",
      "105/105 [==============================] - 0s 333us/step - loss: 0.1313 - acc: 0.9524\n",
      "Epoch 107/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.1271 - acc: 0.9429\n",
      "Epoch 108/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.1222 - acc: 0.9714\n",
      "Epoch 109/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.1344 - acc: 0.9429\n",
      "Epoch 110/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1308 - acc: 0.9333\n",
      "Epoch 111/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.1091 - acc: 0.9810\n",
      "Epoch 112/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1224 - acc: 0.9619\n",
      "Epoch 113/150\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.1189 - acc: 0.9524\n",
      "Epoch 114/150\n",
      "105/105 [==============================] - 0s 428us/step - loss: 0.1260 - acc: 0.9714\n",
      "Epoch 115/150\n",
      "105/105 [==============================] - 0s 447us/step - loss: 0.1188 - acc: 0.9810\n",
      "Epoch 116/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1202 - acc: 0.9429\n",
      "Epoch 117/150\n",
      "105/105 [==============================] - 0s 638us/step - loss: 0.1185 - acc: 0.9619\n",
      "Epoch 118/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1121 - acc: 0.9810\n",
      "Epoch 119/150\n",
      "105/105 [==============================] - 0s 457us/step - loss: 0.1161 - acc: 0.9619\n",
      "Epoch 120/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1082 - acc: 0.9714\n",
      "Epoch 121/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1082 - acc: 0.9619\n",
      "Epoch 122/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.1194 - acc: 0.9619\n",
      "Epoch 123/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1139 - acc: 0.9905\n",
      "Epoch 124/150\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.1130 - acc: 0.9714\n",
      "Epoch 125/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.1180 - acc: 0.9619\n",
      "Epoch 126/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.1101 - acc: 0.9619\n",
      "Epoch 127/150\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.1100 - acc: 0.9810\n",
      "Epoch 128/150\n",
      "105/105 [==============================] - 0s 514us/step - loss: 0.1020 - acc: 0.9714\n",
      "Epoch 129/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1086 - acc: 0.9810\n",
      "Epoch 130/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1033 - acc: 0.9905\n",
      "Epoch 131/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.1050 - acc: 0.9714\n",
      "Epoch 132/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1051 - acc: 0.9714\n",
      "Epoch 133/150\n",
      "105/105 [==============================] - 0s 400us/step - loss: 0.1103 - acc: 0.9810\n",
      "Epoch 134/150\n",
      "105/105 [==============================] - 0s 324us/step - loss: 0.1111 - acc: 0.9524\n",
      "Epoch 135/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.0959 - acc: 0.9714\n",
      "Epoch 136/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1103 - acc: 0.9619\n",
      "Epoch 137/150\n",
      "105/105 [==============================] - 0s 381us/step - loss: 0.1120 - acc: 0.9714\n",
      "Epoch 138/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.1143 - acc: 0.9714\n",
      "Epoch 139/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.1107 - acc: 0.9619\n",
      "Epoch 140/150\n",
      "105/105 [==============================] - 0s 362us/step - loss: 0.1085 - acc: 0.9810\n",
      "Epoch 141/150\n",
      "105/105 [==============================] - 0s 466us/step - loss: 0.0939 - acc: 0.9810\n",
      "Epoch 142/150\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.1059 - acc: 0.9619\n",
      "Epoch 143/150\n",
      "105/105 [==============================] - 0s 428us/step - loss: 0.1087 - acc: 0.9619\n",
      "Epoch 144/150\n",
      "105/105 [==============================] - 0s 371us/step - loss: 0.1084 - acc: 0.9810\n",
      "Epoch 145/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.0966 - acc: 0.9810\n",
      "Epoch 146/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.1070 - acc: 0.9714\n",
      "Epoch 147/150\n",
      "105/105 [==============================] - 0s 305us/step - loss: 0.1033 - acc: 0.9714\n",
      "Epoch 148/150\n",
      "105/105 [==============================] - 0s 409us/step - loss: 0.0978 - acc: 0.9810\n",
      "Epoch 149/150\n",
      "105/105 [==============================] - 0s 343us/step - loss: 0.1079 - acc: 0.9810\n",
      "Epoch 150/150\n",
      "105/105 [==============================] - 0s 352us/step - loss: 0.0976 - acc: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a9da1d4630>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=150, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-SgSSdRcih5"
   },
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBgKZkhkcih6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.33333333333333"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "scores[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P32ASP1Vjt0a"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8rd0jjAjyTR"
   },
   "outputs": [],
   "source": [
    "model.save(\"iris_keras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiipRpe7rbVh"
   },
   "source": [
    "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
    "\n",
    "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Du3lubr4sA"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "105/105 [==============================] - 1s 8ms/step - loss: 1.0508 - acc: 0.3619\n",
      "Epoch 2/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.9554 - acc: 0.4381\n",
      "Epoch 3/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.8955 - acc: 0.4857\n",
      "Epoch 4/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.8269 - acc: 0.6952\n",
      "Epoch 5/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.7731 - acc: 0.8190\n",
      "Epoch 6/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.7297 - acc: 0.7619\n",
      "Epoch 7/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.6941 - acc: 0.7714\n",
      "Epoch 8/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.6332 - acc: 0.8667\n",
      "Epoch 9/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.5935 - acc: 0.7905\n",
      "Epoch 10/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.5539 - acc: 0.9238\n",
      "Epoch 11/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.5245 - acc: 0.8476\n",
      "Epoch 12/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.5044 - acc: 0.8381\n",
      "Epoch 13/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.4756 - acc: 0.8571\n",
      "Epoch 14/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.4468 - acc: 0.8571\n",
      "Epoch 15/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.4292 - acc: 0.9333\n",
      "Epoch 16/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.4188 - acc: 0.8571\n",
      "Epoch 17/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.4090 - acc: 0.8857\n",
      "Epoch 18/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.3869 - acc: 0.9048\n",
      "Epoch 19/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.3700 - acc: 0.8667\n",
      "Epoch 20/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.3659 - acc: 0.8667\n",
      "Epoch 21/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.3603 - acc: 0.9048\n",
      "Epoch 22/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.3590 - acc: 0.8667\n",
      "Epoch 23/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.3377 - acc: 0.9143\n",
      "Epoch 24/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.3562 - acc: 0.8667\n",
      "Epoch 25/150\n",
      "105/105 [==============================] - 0s 267us/step - loss: 0.3358 - acc: 0.9048\n",
      "Epoch 26/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.3158 - acc: 0.9429\n",
      "Epoch 27/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.2814 - acc: 0.9238\n",
      "Epoch 28/150\n",
      "105/105 [==============================] - 0s 286us/step - loss: 0.2905 - acc: 0.9143\n",
      "Epoch 29/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.2841 - acc: 0.9048\n",
      "Epoch 30/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.2902 - acc: 0.9429\n",
      "Epoch 31/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.2752 - acc: 0.9143\n",
      "Epoch 32/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.2953 - acc: 0.8952\n",
      "Epoch 33/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.2515 - acc: 0.9333\n",
      "Epoch 34/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.2602 - acc: 0.9238\n",
      "Epoch 35/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.2623 - acc: 0.9143\n",
      "Epoch 36/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.2133 - acc: 0.9714\n",
      "Epoch 37/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2287 - acc: 0.9333\n",
      "Epoch 38/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.2196 - acc: 0.9524\n",
      "Epoch 39/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.2096 - acc: 0.9333\n",
      "Epoch 40/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.2291 - acc: 0.9429\n",
      "Epoch 41/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.2157 - acc: 0.9429\n",
      "Epoch 42/150\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.2615 - acc: 0.8857\n",
      "Epoch 43/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1824 - acc: 0.9524\n",
      "Epoch 44/150\n",
      "105/105 [==============================] - 0s 276us/step - loss: 0.1983 - acc: 0.9429\n",
      "Epoch 45/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1968 - acc: 0.9429\n",
      "Epoch 46/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1736 - acc: 0.9429\n",
      "Epoch 47/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.2507 - acc: 0.8952\n",
      "Epoch 48/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.1793 - acc: 0.9524\n",
      "Epoch 49/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1912 - acc: 0.9619\n",
      "Epoch 50/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1629 - acc: 0.9619\n",
      "Epoch 51/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1736 - acc: 0.9714\n",
      "Epoch 52/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1494 - acc: 0.9429\n",
      "Epoch 53/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.2058 - acc: 0.9524\n",
      "Epoch 54/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1399 - acc: 0.9524\n",
      "Epoch 55/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1778 - acc: 0.9333\n",
      "Epoch 56/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1589 - acc: 0.9333\n",
      "Epoch 57/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1441 - acc: 0.9714\n",
      "Epoch 58/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1650 - acc: 0.9429\n",
      "Epoch 59/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.2538 - acc: 0.8762\n",
      "Epoch 60/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1662 - acc: 0.9143\n",
      "Epoch 61/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.2083 - acc: 0.9238\n",
      "Epoch 62/150\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.1159 - acc: 0.9714\n",
      "Epoch 63/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3746 - acc: 0.8000\n",
      "Epoch 64/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.4852 - acc: 0.8381\n",
      "Epoch 65/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1758 - acc: 0.9238\n",
      "Epoch 66/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1691 - acc: 0.9429\n",
      "Epoch 67/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1638 - acc: 0.9524\n",
      "Epoch 68/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1470 - acc: 0.9429\n",
      "Epoch 69/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1363 - acc: 0.9619\n",
      "Epoch 70/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1192 - acc: 0.9524\n",
      "Epoch 71/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1318 - acc: 0.9714\n",
      "Epoch 72/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1997 - acc: 0.9238\n",
      "Epoch 73/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1148 - acc: 0.9524\n",
      "Epoch 74/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1148 - acc: 0.9619\n",
      "Epoch 75/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1808 - acc: 0.9143\n",
      "Epoch 76/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1318 - acc: 0.9524\n",
      "Epoch 77/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1247 - acc: 0.9524\n",
      "Epoch 78/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.2279 - acc: 0.9238\n",
      "Epoch 79/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1156 - acc: 0.9714\n",
      "Epoch 80/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1220 - acc: 0.9524\n",
      "Epoch 81/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1373 - acc: 0.9524\n",
      "Epoch 82/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.1468 - acc: 0.9524\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 238us/step - loss: 0.1165 - acc: 0.9524\n",
      "Epoch 84/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1138 - acc: 0.9429\n",
      "Epoch 85/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1288 - acc: 0.9524\n",
      "Epoch 86/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1161 - acc: 0.9619\n",
      "Epoch 87/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0991 - acc: 0.9714\n",
      "Epoch 88/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.3450 - acc: 0.8571\n",
      "Epoch 89/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1362 - acc: 0.9619\n",
      "Epoch 90/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1028 - acc: 0.9619\n",
      "Epoch 91/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1115 - acc: 0.9524\n",
      "Epoch 92/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1090 - acc: 0.9429\n",
      "Epoch 93/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1264 - acc: 0.9524\n",
      "Epoch 94/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1739 - acc: 0.9238\n",
      "Epoch 95/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1115 - acc: 0.9619\n",
      "Epoch 96/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.0789 - acc: 0.9810\n",
      "Epoch 97/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1042 - acc: 0.9619\n",
      "Epoch 98/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.0999 - acc: 0.9810\n",
      "Epoch 99/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1481 - acc: 0.9429\n",
      "Epoch 100/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1064 - acc: 0.9714\n",
      "Epoch 101/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1928 - acc: 0.9429\n",
      "Epoch 102/150\n",
      "105/105 [==============================] - 0s 276us/step - loss: 0.5145 - acc: 0.8286\n",
      "Epoch 103/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1256 - acc: 0.9619\n",
      "Epoch 104/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1027 - acc: 0.9714\n",
      "Epoch 105/150\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.1176 - acc: 0.9524\n",
      "Epoch 106/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.0976 - acc: 0.9619\n",
      "Epoch 107/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1584 - acc: 0.9048\n",
      "Epoch 108/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1057 - acc: 0.9524\n",
      "Epoch 109/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.0769 - acc: 0.9810\n",
      "Epoch 110/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.0934 - acc: 0.9619\n",
      "Epoch 111/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1514 - acc: 0.9238\n",
      "Epoch 112/150\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.1523 - acc: 0.9429\n",
      "Epoch 113/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1119 - acc: 0.9619\n",
      "Epoch 114/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1443 - acc: 0.9429\n",
      "Epoch 115/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1471 - acc: 0.9238\n",
      "Epoch 116/150\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.1358 - acc: 0.9429\n",
      "Epoch 117/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.1662 - acc: 0.9238\n",
      "Epoch 118/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1232 - acc: 0.9619\n",
      "Epoch 119/150\n",
      "105/105 [==============================] - 0s 190us/step - loss: 0.1114 - acc: 0.9524\n",
      "Epoch 120/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0989 - acc: 0.9524\n",
      "Epoch 121/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0927 - acc: 0.9619\n",
      "Epoch 122/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.1566 - acc: 0.9429\n",
      "Epoch 123/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.0766 - acc: 0.9810\n",
      "Epoch 124/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1991 - acc: 0.9143\n",
      "Epoch 125/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.0918 - acc: 0.9619\n",
      "Epoch 126/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1145 - acc: 0.9238\n",
      "Epoch 127/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.0836 - acc: 0.9810\n",
      "Epoch 128/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.1237 - acc: 0.9619\n",
      "Epoch 129/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.0906 - acc: 0.9619\n",
      "Epoch 130/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.1701 - acc: 0.9333\n",
      "Epoch 131/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.0840 - acc: 0.9810\n",
      "Epoch 132/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1805 - acc: 0.9333\n",
      "Epoch 133/150\n",
      "105/105 [==============================] - 0s 228us/step - loss: 0.0952 - acc: 0.9524\n",
      "Epoch 134/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1668 - acc: 0.9143\n",
      "Epoch 135/150\n",
      "105/105 [==============================] - 0s 286us/step - loss: 0.0760 - acc: 0.9714\n",
      "Epoch 136/150\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.1613 - acc: 0.9048\n",
      "Epoch 137/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0983 - acc: 0.9619\n",
      "Epoch 138/150\n",
      "105/105 [==============================] - 0s 162us/step - loss: 0.0849 - acc: 0.9905\n",
      "Epoch 139/150\n",
      "105/105 [==============================] - 0s 219us/step - loss: 0.2055 - acc: 0.9238\n",
      "Epoch 140/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.1035 - acc: 0.9619\n",
      "Epoch 141/150\n",
      "105/105 [==============================] - 0s 152us/step - loss: 0.1041 - acc: 0.9524\n",
      "Epoch 142/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.0723 - acc: 0.9810\n",
      "Epoch 143/150\n",
      "105/105 [==============================] - 0s 238us/step - loss: 0.3062 - acc: 0.8952\n",
      "Epoch 144/150\n",
      "105/105 [==============================] - 0s 171us/step - loss: 0.0791 - acc: 0.9619\n",
      "Epoch 145/150\n",
      "105/105 [==============================] - 0s 248us/step - loss: 0.1258 - acc: 0.9524\n",
      "Epoch 146/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.0713 - acc: 0.9810\n",
      "Epoch 147/150\n",
      "105/105 [==============================] - 0s 181us/step - loss: 0.1212 - acc: 0.9524\n",
      "Epoch 148/150\n",
      "105/105 [==============================] - 0s 209us/step - loss: 0.0794 - acc: 0.9524\n",
      "Epoch 149/150\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.1007 - acc: 0.9524\n",
      "Epoch 150/150\n",
      "105/105 [==============================] - 0s 200us/step - loss: 0.1077 - acc: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a9da1f36a0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.77777777777777"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "scores[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes additional hidden layers is improving the performance due to additional neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3. R6_InternalLab_AIML_Share_Prices-Eager Execution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
